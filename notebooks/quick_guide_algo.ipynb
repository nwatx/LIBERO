{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we walk through all the necessary components of running experiments on LIBERO, and some common usage such as defining your own algorithm and policy architectures in the codebase.\n",
    "\n",
    "1. Dataset preparation for your algorithms\n",
    "2. Write your own algorithm\n",
    "    - Subclassing from `Sequential` base class\n",
    "3. Write your own model\n",
    "4. Write your training loop\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16909/2982132737.py:18: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../libero/configs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'color_aug': { 'network': 'BatchWiseImgColorJitterAug',\n",
      "                 'network_kwargs': { 'brightness': 0.3,\n",
      "                                     'contrast': 0.3,\n",
      "                                     'epsilon': 0.1,\n",
      "                                     'hue': 0.3,\n",
      "                                     'input_shape': None,\n",
      "                                     'saturation': 0.3}},\n",
      "  'embed_size': 64,\n",
      "  'extra_hidden_size': 128,\n",
      "  'extra_num_layers': 0,\n",
      "  'image_encoder': { 'network': 'ResnetEncoder',\n",
      "                     'network_kwargs': { 'freeze': False,\n",
      "                                         'language_fusion': 'film',\n",
      "                                         'no_stride': False,\n",
      "                                         'pretrained': False,\n",
      "                                         'remove_layer_num': 4}},\n",
      "  'language_encoder': { 'network': 'MLPEncoder',\n",
      "                        'network_kwargs': { 'hidden_size': 128,\n",
      "                                            'input_size': 768,\n",
      "                                            'num_layers': 1,\n",
      "                                            'output_size': 128}},\n",
      "  'policy_head': { 'loss_kwargs': {'loss_coef': 1.0},\n",
      "                   'network': 'GMMHead',\n",
      "                   'network_kwargs': { 'activation': 'softplus',\n",
      "                                       'hidden_size': 1024,\n",
      "                                       'low_eval_noise': False,\n",
      "                                       'min_std': 0.0001,\n",
      "                                       'num_layers': 2,\n",
      "                                       'num_modes': 5}},\n",
      "  'policy_type': 'BCTransformerPolicy',\n",
      "  'temporal_position_encoding': { 'network': 'SinusoidalPositionEncoding',\n",
      "                                  'network_kwargs': { 'factor_ratio': None,\n",
      "                                                      'input_size': None,\n",
      "                                                      'inv_freq_factor': 10}},\n",
      "  'transformer_dropout': 0.1,\n",
      "  'transformer_head_output_size': 64,\n",
      "  'transformer_input_size': None,\n",
      "  'transformer_max_seq_len': 10,\n",
      "  'transformer_mlp_hidden_size': 256,\n",
      "  'transformer_num_heads': 6,\n",
      "  'transformer_num_layers': 4,\n",
      "  'translation_aug': { 'network': 'TranslationAug',\n",
      "                       'network_kwargs': { 'input_shape': None,\n",
      "                                           'translation': 8}}}\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: rgb with keys: ['eye_in_hand_rgb', 'agentview_rgb']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: low_dim with keys: ['joint_states', 'gripper_states']\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 514.76it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 586.10it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 550.93it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 582.78it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 572.78it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 510.45it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 570.99it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 551.66it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 590.31it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 530.95it/s]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025896549224853516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 29,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea0d25aa10740caa8ba1b4618738d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0220797061920166,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0048288afbb4f4db8a3c14b1a92609a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019705772399902344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 213450,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee0c1ebe364cd9a12b3fc3488244a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01690530776977539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 435797,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e77547c1f7a4f1bb508bccb2f63c825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0184328556060791,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 435779157,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db69199e65744963b60e7ba19fbdd8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "import hydra\n",
    "import pprint\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.lifelong.datasets import (GroupedTaskDataset, SequenceVLDataset, get_dataset)\n",
    "from libero.lifelong.utils import (get_task_embs, safe_device, create_experiment_dir)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "### load the default hydra config\n",
    "initialize(config_path=\"../libero/configs\")\n",
    "hydra_cfg = compose(config_name=\"config\")\n",
    "yaml_config = OmegaConf.to_yaml(hydra_cfg)\n",
    "cfg = EasyDict(yaml.safe_load(yaml_config))\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(cfg.policy)\n",
    "\n",
    "# prepare lifelong learning\n",
    "cfg.folder = get_libero_path(\"datasets\")\n",
    "cfg.bddl_folder = get_libero_path(\"bddl_files\")\n",
    "cfg.init_states_folder = get_libero_path(\"init_states\")\n",
    "cfg.eval.num_procs = 1\n",
    "cfg.eval.n_eval = 1\n",
    "\n",
    "task_order = cfg.data.task_order_index # can be from {0 .. 21}, default to 0, which is [task 0, 1, 2 ...]\n",
    "cfg.benchmark_name = \"libero_spatial\" # can be from {\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_10\"}\n",
    "benchmark = get_benchmark(cfg.benchmark_name)(task_order)\n",
    "\n",
    "# prepare datasets from the benchmark\n",
    "datasets = []\n",
    "descriptions = []\n",
    "shape_meta = None\n",
    "n_tasks = benchmark.n_tasks\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    # currently we assume tasks from same benchmark have the same shape_meta\n",
    "    task_i_dataset, shape_meta = get_dataset(\n",
    "            dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)),\n",
    "            obs_modality=cfg.data.obs.modality,\n",
    "            initialize_obs_utils=(i==0),\n",
    "            seq_len=cfg.data.seq_len,\n",
    "    )\n",
    "    # add language to the vision dataset, hence we call vl_dataset\n",
    "    descriptions.append(benchmark.get_task(i).language)\n",
    "    datasets.append(task_i_dataset)\n",
    "\n",
    "task_embs = get_task_embs(cfg, descriptions)\n",
    "benchmark.set_task_embs(task_embs)\n",
    "\n",
    "datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)]\n",
    "n_demos = [data.n_demos for data in datasets]\n",
    "n_sequences = [data.total_num_sequences for data in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write your own policy architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T16:31:09.072132263Z",
     "start_time": "2023-09-06T16:31:06.020918932Z"
    }
   },
   "outputs": [],
   "source": [
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from libero.lifelong.models.modules.rgb_modules import *\n",
    "from libero.lifelong.models.modules.language_modules import *\n",
    "from libero.lifelong.models.base_policy import BasePolicy\n",
    "from libero.lifelong.models.policy_head import *\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A model handling extra input modalities besides images at time t.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "class ExtraModalities:\n",
    "    def __init__(self,\n",
    "                 use_joint=False,\n",
    "                 use_gripper=False,\n",
    "                 use_ee=False,\n",
    "                 extra_hidden_size=64,\n",
    "                 extra_embedding_size=32):\n",
    "\n",
    "        self.use_joint = use_joint\n",
    "        self.use_gripper = use_gripper\n",
    "        self.use_ee = use_ee\n",
    "        self.extra_embedding_size = extra_embedding_size\n",
    "\n",
    "        joint_states_dim = 7\n",
    "        gripper_states_dim = 2\n",
    "        ee_dim = 6\n",
    "\n",
    "        self.extra_low_level_feature_dim = int(use_joint) * joint_states_dim + \\\n",
    "                int(use_gripper) * gripper_states_dim + \\\n",
    "                int(use_ee) * ee_dim\n",
    "        assert self.extra_low_level_feature_dim > 0, \"[error] no extra information\"\n",
    "\n",
    "    def __call__(self, obs_dict):\n",
    "        \"\"\"\n",
    "        obs_dict: {\n",
    "            (optional) joint_stats: (B, T, 7),\n",
    "            (optional) gripper_states: (B, T, 2),\n",
    "            (optional) ee: (B, T, 3) \n",
    "        }\n",
    "        map above to a latent vector of shape (B, T, H)\n",
    "        \"\"\"\n",
    "        tensor_list = []\n",
    "        if self.use_joint:\n",
    "            tensor_list.append(obs_dict[\"joint_states\"])\n",
    "        if self.use_gripper:\n",
    "            tensor_list.append(obs_dict[\"gripper_states\"])\n",
    "        if self.use_ee:\n",
    "            tensor_list.append(obs_dict[\"ee_states\"])   \n",
    "        x = torch.cat(tensor_list, dim=-1)\n",
    "        return x\n",
    "        \n",
    "    def output_shape(self, input_shape, shape_meta):\n",
    "        return (self.extra_low_level_feature_dim,)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A RNN policy\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "class MyRNNPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Input: (o_{t-H}, ... , o_t)\n",
    "    Output: a_t or distribution of a_t\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cfg, \n",
    "                 shape_meta):\n",
    "        super().__init__(cfg, shape_meta)\n",
    "        policy_cfg = cfg.policy\n",
    "\n",
    "        ### 1. encode image\n",
    "        rnn_input_size = 0\n",
    "        image_embed_size = 64\n",
    "        self.image_encoders = {}\n",
    "        for name in shape_meta[\"all_shapes\"].keys():\n",
    "            if \"rgb\" in name or \"depth\" in name:\n",
    "                kwargs = policy_cfg.image_encoder.network_kwargs\n",
    "                kwargs.input_shape = shape_meta[\"all_shapes\"][name]\n",
    "                kwargs.output_size = image_embed_size \n",
    "                kwargs.language_dim = policy_cfg.language_encoder.network_kwargs.input_size\n",
    "                self.image_encoders[name] = {\n",
    "                    \"input_shape\": shape_meta[\"all_shapes\"][name],\n",
    "                    \"encoder\": eval(policy_cfg.image_encoder.network)(**kwargs)\n",
    "                }\n",
    "                rnn_input_size += image_embed_size\n",
    "        self.encoders = nn.ModuleList([x[\"encoder\"] for x in self.image_encoders.values()])\n",
    "\n",
    "        ### 2. encode language\n",
    "        text_embed_size = 32\n",
    "        policy_cfg.language_encoder.network_kwargs.output_size = text_embed_size\n",
    "        self.language_encoder = eval(policy_cfg.language_encoder.network)(\n",
    "                **policy_cfg.language_encoder.network_kwargs)\n",
    "        rnn_input_size += text_embed_size\n",
    "\n",
    "        ### 3. encode extra information (e.g. gripper, joint_state)\n",
    "        self.extra_encoder = ExtraModalities(\n",
    "                 use_joint=cfg.data.use_joint,\n",
    "                 use_gripper=cfg.data.use_gripper,\n",
    "                 use_ee=cfg.data.use_ee)\n",
    "        rnn_input_size += self.extra_encoder.extra_low_level_feature_dim\n",
    "        bidirectional = False\n",
    "        self.rnn = nn.LSTM(input_size=rnn_input_size,\n",
    "                           hidden_size=1024,\n",
    "                           num_layers=2,\n",
    "                           batch_first=True,\n",
    "                           dropout=0.0,\n",
    "                           bidirectional=bidirectional)\n",
    "\n",
    "        ### 4. use policy head to output action\n",
    "        self.D = 2 if bidirectional else 1\n",
    "        self.policy_head = GMMHead(\n",
    "                input_size=1024,\n",
    "                loss_coef=2.0,\n",
    "                hidden_size=1024,\n",
    "                num_layers=2,\n",
    "                min_std=0.0001,\n",
    "                num_modes=5,\n",
    "                activation=\"softplus\",\n",
    "                output_size=shape_meta[\"ac_dim\"])\n",
    "        self.eval_h0 = None\n",
    "        self.eval_c0 = None\n",
    "\n",
    "    def forward(self, data, train_mode=True):\n",
    "        # 1. encode image\n",
    "        encoded = []\n",
    "        for img_name in self.image_encoders.keys():\n",
    "            x = data[\"obs\"][img_name]\n",
    "            B, T, C, H, W = x.shape\n",
    "            e = self.image_encoders[img_name][\"encoder\"](\n",
    "                    x.reshape(B*T, C, H, W),\n",
    "                    langs=data[\"task_emb\"].reshape(B,1,-1).repeat(1,T,1).reshape(B*T, -1)\n",
    "            ).view(B,T,-1)\n",
    "            encoded.append(e)\n",
    "\n",
    "        # 2. add joint states, gripper info, etc.\n",
    "        encoded.append(self.extra_encoder(data[\"obs\"])) # add (B, T, H_extra)\n",
    "        encoded = torch.cat(encoded, -1) # (B, T, H_all)\n",
    "\n",
    "        # 3. language encoding\n",
    "        lang_h = self.language_encoder(data) # (B, H)\n",
    "        encoded = torch.cat([encoded,\n",
    "            lang_h.unsqueeze(1).expand(-1, encoded.shape[1], -1)], dim=-1)\n",
    "\n",
    "        # 4. apply temporal rnn\n",
    "        if train_mode:\n",
    "            h0 = torch.zeros(self.D * 2,\n",
    "                             encoded.shape[0],\n",
    "                             1024).to(self.device)\n",
    "            c0 = torch.zeros(self.D * 2,\n",
    "                             encoded.shape[0],\n",
    "                             1024).to(self.device)\n",
    "            output, (hn, cn) = self.rnn(encoded, (h0, c0))\n",
    "        else:\n",
    "            if self.eval_h0 is None:\n",
    "                self.eval_h0 = torch.zeros(\n",
    "                        self.D * 2,\n",
    "                        encoded.shape[0],\n",
    "                        1024).to(self.device)\n",
    "                self.eval_c0 = torch.zeros(\n",
    "                        self.D * 2,\n",
    "                        encoded.shape[0],\n",
    "                        1024).to(self.device)\n",
    "            output, (h1, c1) = self.rnn(encoded, (self.eval_h0, self.eval_c0))\n",
    "            self.eval_h0 = h1.detach()\n",
    "            self.eval_c0 = c1.detach()\n",
    "\n",
    "        dist = self.policy_head(output)\n",
    "        return dist\n",
    "    \n",
    "    def get_action(self, data):\n",
    "        self.eval()\n",
    "        data = self.preprocess_input(data, train_mode=False)\n",
    "        with torch.no_grad():\n",
    "            dist = self.forward(data)\n",
    "        action = dist.sample().detach().cpu()\n",
    "        return action.view(action.shape[0], -1).numpy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.eval_h0 = None\n",
    "        self.eval_c0 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write your own lifelong learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[33m[robosuite WARNING] \u001B[0mNo private macro file found! (__init__.py:7)\n",
      "\u001B[1m\u001B[33m[robosuite WARNING] \u001B[0mIt is recommended to use a private macro file (__init__.py:8)\n",
      "\u001B[1m\u001B[33m[robosuite WARNING] \u001B[0mTo setup, run: python /home/neo/.local/lib/python3.8/site-packages/robosuite/scripts/setup_macros.py (__init__.py:9)\n"
     ]
    }
   ],
   "source": [
    "from libero.lifelong.algos.base import Sequential\n",
    "\n",
    "### All lifelong learning algorithm should inherit the Sequential algorithm super class\n",
    "\n",
    "class MyLifelongAlgo(Sequential):\n",
    "    \"\"\"\n",
    "    The experience replay policy.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_tasks,\n",
    "                 cfg,\n",
    "                 **policy_kwargs):\n",
    "        super().__init__(n_tasks=n_tasks, cfg=cfg, **policy_kwargs)\n",
    "        # define the learning policy\n",
    "        self.policy = MyRNNPolicy(cfg, cfg.shape_meta)\n",
    "\n",
    "    def start_task(self, task):\n",
    "        # what to do at the beginning of a new task\n",
    "        super().start_task(task)\n",
    "\n",
    "    def end_task(self, dataset, task_id, benchmark):\n",
    "        # what to do when finish learning a new task\n",
    "        self.datasets.append(dataset)\n",
    "\n",
    "    def observe(self, data):\n",
    "        # how the algorithm observes a data and returns a loss to be optimized\n",
    "        loss = super().observe(data)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write your training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_experiment_dir(cfg)\n",
    "cfg.shape_meta = shape_meta\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"experiment directory is: \", cfg.experiment_dir)\n",
    "algo = safe_device(MyLifelongAlgo(n_tasks, cfg), cfg.device)\n",
    "\n",
    "result_summary = {\n",
    "    'L_conf_mat': np.zeros((n_tasks, n_tasks)),   # loss confusion matrix\n",
    "    'S_conf_mat': np.zeros((n_tasks, n_tasks)),   # success confusion matrix\n",
    "    'L_fwd'     : np.zeros((n_tasks,)),           # loss AUC, how fast the agent learns\n",
    "    'S_fwd'     : np.zeros((n_tasks,)),           # success AUC, how fast the agent succeeds\n",
    "}\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    algo.train()\n",
    "    s_fwd, l_fwd = algo.learn_one_task(datasets[i], i, benchmark, result_summary)\n",
    "    # s_fwd is success rate AUC, when the agent learns the {0, e, 2e, ...} epochs\n",
    "    # l_fwd is BC loss AUC, similar to s_fwd\n",
    "    result_summary[\"S_fwd\"][i] = s_fwd\n",
    "    result_summary[\"L_fwd\"][i] = l_fwd\n",
    "\n",
    "    if cfg.eval.eval:\n",
    "        algo.eval()\n",
    "        # we only evaluate on the past tasks: 0 .. i\n",
    "        L = evaluate_loss(cfg, algo, benchmark, datasets[:i+1]) # (i+1,)\n",
    "        S = evaluate_success(cfg, algo, benchmark, list(range((i+1)*gsz))) # (i+1,)\n",
    "        result_summary[\"L_conf_mat\"][i][:i+1] = L\n",
    "        result_summary[\"S_conf_mat\"][i][:i+1] = S\n",
    "\n",
    "        torch.save(result_summary, os.path.join(cfg.experiment_dir, f'result.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize loss and success rate curves on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize policy rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
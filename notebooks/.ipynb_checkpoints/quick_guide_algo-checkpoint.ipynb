{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we walk through all the necessary components of running experiments on LIBERO, and some common usage such as defining your own algorithm and policy architectures in the codebase.\n",
    "\n",
    "1. Dataset preparation for your algorithms\n",
    "2. Write your own algorithm\n",
    "    - Subclassing from `Sequential` base class\n",
    "3. Write your own model\n",
    "4. Write your training loop\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bo/miniconda3/envs/libero/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_29210/2982132737.py:18: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../libero/configs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'color_aug': { 'network': 'BatchWiseImgColorJitterAug',\n",
      "                 'network_kwargs': { 'brightness': 0.3,\n",
      "                                     'contrast': 0.3,\n",
      "                                     'epsilon': 0.1,\n",
      "                                     'hue': 0.3,\n",
      "                                     'input_shape': None,\n",
      "                                     'saturation': 0.3}},\n",
      "  'embed_size': 64,\n",
      "  'extra_hidden_size': 128,\n",
      "  'extra_num_layers': 0,\n",
      "  'image_encoder': { 'network': 'ResnetEncoder',\n",
      "                     'network_kwargs': { 'freeze': False,\n",
      "                                         'language_fusion': 'film',\n",
      "                                         'no_stride': False,\n",
      "                                         'pretrained': False,\n",
      "                                         'remove_layer_num': 4}},\n",
      "  'language_encoder': { 'network': 'MLPEncoder',\n",
      "                        'network_kwargs': { 'hidden_size': 128,\n",
      "                                            'input_size': 768,\n",
      "                                            'num_layers': 1,\n",
      "                                            'output_size': 128}},\n",
      "  'policy_head': { 'loss_kwargs': {'loss_coef': 1.0},\n",
      "                   'network': 'GMMHead',\n",
      "                   'network_kwargs': { 'activation': 'softplus',\n",
      "                                       'hidden_size': 1024,\n",
      "                                       'low_eval_noise': False,\n",
      "                                       'min_std': 0.0001,\n",
      "                                       'num_layers': 2,\n",
      "                                       'num_modes': 5}},\n",
      "  'policy_type': 'BCTransformerPolicy',\n",
      "  'temporal_position_encoding': { 'network': 'SinusoidalPositionEncoding',\n",
      "                                  'network_kwargs': { 'factor_ratio': None,\n",
      "                                                      'input_size': None,\n",
      "                                                      'inv_freq_factor': 10}},\n",
      "  'transformer_dropout': 0.1,\n",
      "  'transformer_head_output_size': 64,\n",
      "  'transformer_input_size': None,\n",
      "  'transformer_max_seq_len': 10,\n",
      "  'transformer_mlp_hidden_size': 256,\n",
      "  'transformer_num_heads': 6,\n",
      "  'transformer_num_layers': 4,\n",
      "  'translation_aug': { 'network': 'TranslationAug',\n",
      "                       'network_kwargs': { 'input_shape': None,\n",
      "                                           'translation': 8}}}\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: rgb with keys: ['agentview_rgb', 'eye_in_hand_rgb']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: low_dim with keys: ['joint_states', 'gripper_states']\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1525.95it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1619.25it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1620.77it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1612.07it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1569.42it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1577.85it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1597.16it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1596.94it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1600.20it/s]\n",
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 50/50 [00:00<00:00, 1598.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "\n",
    "from libero.libero import benchmark, get_libero_path\n",
    "import hydra\n",
    "import pprint\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.lifelong.datasets import (GroupedTaskDataset, SequenceVLDataset, get_dataset)\n",
    "from libero.lifelong.utils import (get_task_embs, safe_device, create_experiment_dir)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "### load the default hydra config\n",
    "initialize(config_path=\"../libero/configs\")\n",
    "hydra_cfg = compose(config_name=\"config\")\n",
    "yaml_config = OmegaConf.to_yaml(hydra_cfg)\n",
    "cfg = EasyDict(yaml.safe_load(yaml_config))\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(cfg.policy)\n",
    "\n",
    "# prepare lifelong learning\n",
    "cfg.folder = get_libero_path(\"datasets\")\n",
    "cfg.bddl_folder = get_libero_path(\"bddl_files\")\n",
    "cfg.init_states_folder = get_libero_path(\"init_states\")\n",
    "cfg.eval.num_procs = 1\n",
    "cfg.eval.n_eval = 1\n",
    "\n",
    "task_order = cfg.data.task_order_index # can be from {0 .. 21}, default to 0, which is [task 0, 1, 2 ...]\n",
    "cfg.benchmark_name = \"libero_spatial\" # can be from {\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_10\"}\n",
    "benchmark = get_benchmark(cfg.benchmark_name)(task_order)\n",
    "\n",
    "# prepare datasets from the benchmark\n",
    "datasets = []\n",
    "descriptions = []\n",
    "shape_meta = None\n",
    "n_tasks = benchmark.n_tasks\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    # currently we assume tasks from same benchmark have the same shape_meta\n",
    "    task_i_dataset, shape_meta = get_dataset(\n",
    "            dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)),\n",
    "            obs_modality=cfg.data.obs.modality,\n",
    "            initialize_obs_utils=(i==0),\n",
    "            seq_len=cfg.data.seq_len,\n",
    "    )\n",
    "    # add language to the vision dataset, hence we call vl_dataset\n",
    "    descriptions.append(benchmark.get_task(i).language)\n",
    "    datasets.append(task_i_dataset)\n",
    "\n",
    "task_embs = get_task_embs(cfg, descriptions)\n",
    "benchmark.set_task_embs(task_embs)\n",
    "\n",
    "datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)]\n",
    "n_demos = [data.n_demos for data in datasets]\n",
    "n_sequences = [data.total_num_sequences for data in datasets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write your own policy architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from libero.lifelong.models.modules.rgb_modules import *\n",
    "from libero.lifelong.models.modules.language_modules import *\n",
    "from libero.lifelong.models.base_policy import BasePolicy\n",
    "from libero.lifelong.models.policy_head import *\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A model handling extra input modalities besides images at time t.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "class ExtraModalities:\n",
    "    def __init__(self,\n",
    "                 use_joint=False,\n",
    "                 use_gripper=False,\n",
    "                 use_ee=False,\n",
    "                 extra_hidden_size=64,\n",
    "                 extra_embedding_size=32):\n",
    "\n",
    "        self.use_joint = use_joint\n",
    "        self.use_gripper = use_gripper\n",
    "        self.use_ee = use_ee\n",
    "        self.extra_embedding_size = extra_embedding_size\n",
    "\n",
    "        joint_states_dim = 7\n",
    "        gripper_states_dim = 2\n",
    "        ee_dim = 6\n",
    "\n",
    "        self.extra_low_level_feature_dim = int(use_joint) * joint_states_dim + \\\n",
    "                int(use_gripper) * gripper_states_dim + \\\n",
    "                int(use_ee) * ee_dim\n",
    "        assert self.extra_low_level_feature_dim > 0, \"[error] no extra information\"\n",
    "\n",
    "    def __call__(self, obs_dict):\n",
    "        \"\"\"\n",
    "        obs_dict: {\n",
    "            (optional) joint_stats: (B, T, 7),\n",
    "            (optional) gripper_states: (B, T, 2),\n",
    "            (optional) ee: (B, T, 3) \n",
    "        }\n",
    "        map above to a latent vector of shape (B, T, H)\n",
    "        \"\"\"\n",
    "        tensor_list = []\n",
    "        if self.use_joint:\n",
    "            tensor_list.append(obs_dict[\"joint_states\"])\n",
    "        if self.use_gripper:\n",
    "            tensor_list.append(obs_dict[\"gripper_states\"])\n",
    "        if self.use_ee:\n",
    "            tensor_list.append(obs_dict[\"ee_states\"])   \n",
    "        x = torch.cat(tensor_list, dim=-1)\n",
    "        return x\n",
    "        \n",
    "    def output_shape(self, input_shape, shape_meta):\n",
    "        return (self.extra_low_level_feature_dim,)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# A RNN policy\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "class MyRNNPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Input: (o_{t-H}, ... , o_t)\n",
    "    Output: a_t or distribution of a_t\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cfg, \n",
    "                 shape_meta):\n",
    "        super().__init__(cfg, shape_meta)\n",
    "        policy_cfg = cfg.policy\n",
    "\n",
    "        ### 1. encode image\n",
    "        rnn_input_size = 0\n",
    "        image_embed_size = 64\n",
    "        self.image_encoders = {}\n",
    "        for name in shape_meta[\"all_shapes\"].keys():\n",
    "            if \"rgb\" in name or \"depth\" in name:\n",
    "                kwargs = policy_cfg.image_encoder.network_kwargs\n",
    "                kwargs.input_shape = shape_meta[\"all_shapes\"][name]\n",
    "                kwargs.output_size = image_embed_size \n",
    "                kwargs.language_dim = policy_cfg.language_encoder.network_kwargs.input_size\n",
    "                self.image_encoders[name] = {\n",
    "                    \"input_shape\": shape_meta[\"all_shapes\"][name],\n",
    "                    \"encoder\": eval(policy_cfg.image_encoder.network)(**kwargs)\n",
    "                }\n",
    "                rnn_input_size += image_embed_size\n",
    "        self.encoders = nn.ModuleList([x[\"encoder\"] for x in self.image_encoders.values()])\n",
    "\n",
    "        ### 2. encode language\n",
    "        text_embed_size = 32\n",
    "        policy_cfg.language_encoder.network_kwargs.output_size = text_embed_size\n",
    "        self.language_encoder = eval(policy_cfg.language_encoder.network)(\n",
    "                **policy_cfg.language_encoder.network_kwargs)\n",
    "        rnn_input_size += text_embed_size\n",
    "\n",
    "        ### 3. encode extra information (e.g. gripper, joint_state)\n",
    "        self.extra_encoder = ExtraModalities(\n",
    "                 use_joint=cfg.data.use_joint,\n",
    "                 use_gripper=cfg.data.use_gripper,\n",
    "                 use_ee=cfg.data.use_ee)\n",
    "        rnn_input_size += self.extra_encoder.extra_low_level_feature_dim\n",
    "        bidirectional = False\n",
    "        self.rnn = nn.LSTM(input_size=rnn_input_size,\n",
    "                           hidden_size=1024,\n",
    "                           num_layers=2,\n",
    "                           batch_first=True,\n",
    "                           dropout=0.0,\n",
    "                           bidirectional=bidirectional)\n",
    "\n",
    "        ### 4. use policy head to output action\n",
    "        self.D = 2 if bidirectional else 1\n",
    "        self.policy_head = GMMHead(\n",
    "                input_size=1024,\n",
    "                loss_coef=2.0,\n",
    "                hidden_size=1024,\n",
    "                num_layers=2,\n",
    "                min_std=0.0001,\n",
    "                num_modes=5,\n",
    "                activation=\"softplus\",\n",
    "                output_size=shape_meta[\"ac_dim\"])\n",
    "        self.eval_h0 = None\n",
    "        self.eval_c0 = None\n",
    "\n",
    "    def forward(self, data, train_mode=True):\n",
    "        # 1. encode image\n",
    "        encoded = []\n",
    "        for img_name in self.image_encoders.keys():\n",
    "            x = data[\"obs\"][img_name]\n",
    "            B, T, C, H, W = x.shape\n",
    "            e = self.image_encoders[img_name][\"encoder\"](\n",
    "                    x.reshape(B*T, C, H, W),\n",
    "                    langs=data[\"task_emb\"].reshape(B,1,-1).repeat(1,T,1).reshape(B*T, -1)\n",
    "            ).view(B,T,-1)\n",
    "            encoded.append(e)\n",
    "\n",
    "        # 2. add joint states, gripper info, etc.\n",
    "        encoded.append(self.extra_encoder(data[\"obs\"])) # add (B, T, H_extra)\n",
    "        encoded = torch.cat(encoded, -1) # (B, T, H_all)\n",
    "\n",
    "        # 3. language encoding\n",
    "        lang_h = self.language_encoder(data) # (B, H)\n",
    "        encoded = torch.cat([encoded,\n",
    "            lang_h.unsqueeze(1).expand(-1, encoded.shape[1], -1)], dim=-1)\n",
    "\n",
    "        # 4. apply temporal rnn\n",
    "        if train_mode:\n",
    "            h0 = torch.zeros(self.D * 2,\n",
    "                             encoded.shape[0],\n",
    "                             1024).to(self.device)\n",
    "            c0 = torch.zeros(self.D * 2,\n",
    "                             encoded.shape[0],\n",
    "                             1024).to(self.device)\n",
    "            output, (hn, cn) = self.rnn(encoded, (h0, c0))\n",
    "        else:\n",
    "            if self.eval_h0 is None:\n",
    "                self.eval_h0 = torch.zeros(\n",
    "                        self.D * 2,\n",
    "                        encoded.shape[0],\n",
    "                        1024).to(self.device)\n",
    "                self.eval_c0 = torch.zeros(\n",
    "                        self.D * 2,\n",
    "                        encoded.shape[0],\n",
    "                        1024).to(self.device)\n",
    "            output, (h1, c1) = self.rnn(encoded, (self.eval_h0, self.eval_c0))\n",
    "            self.eval_h0 = h1.detach()\n",
    "            self.eval_c0 = c1.detach()\n",
    "\n",
    "        dist = self.policy_head(output)\n",
    "        return dist\n",
    "    \n",
    "    def get_action(self, data):\n",
    "        self.eval()\n",
    "        data = self.preprocess_input(data, train_mode=False)\n",
    "        with torch.no_grad():\n",
    "            dist = self.forward(data)\n",
    "        action = dist.sample().detach().cpu()\n",
    "        return action.view(action.shape[0], -1).numpy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.eval_h0 = None\n",
    "        self.eval_c0 = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write your own lifelong learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/bo/Desktop/robosuite/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    }
   ],
   "source": [
    "from libero.lifelong.algos.base import Sequential\n",
    "\n",
    "### All lifelong learning algorithm should inherit the Sequential algorithm super class\n",
    "\n",
    "class MyLifelongAlgo(Sequential):\n",
    "    \"\"\"\n",
    "    The experience replay policy.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_tasks,\n",
    "                 cfg,\n",
    "                 **policy_kwargs):\n",
    "        super().__init__(n_tasks=n_tasks, cfg=cfg, **policy_kwargs)\n",
    "        # define the learning policy\n",
    "        self.policy = MyRNNPolicy(cfg, cfg.shape_meta)\n",
    "\n",
    "    def start_task(self, task):\n",
    "        # what to do at the beginning of a new task\n",
    "        super().start_task(task)\n",
    "\n",
    "    def end_task(self, dataset, task_id, benchmark):\n",
    "        # what to do when finish learning a new task\n",
    "        self.datasets.append(dataset)\n",
    "\n",
    "    def observe(self, data):\n",
    "        # how the algorithm observes a data and returns a loss to be optimized\n",
    "        loss = super().observe(data)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write your training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment directory is:  ./experiments/libero_spatial/Sequential/BCTransformerPolicy_seed10000/run_024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bo/miniconda3/envs/libero/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/bo/miniconda3/envs/libero/lib/python3.8/site-packages/robomimic/utils/dataset.py:516: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pad_mask = pad_mask[:, None].astype(np.bool)\n",
      "/home/bo/miniconda3/envs/libero/lib/python3.8/site-packages/robomimic/utils/dataset.py:516: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pad_mask = pad_mask[:, None].astype(np.bool)\n",
      "/home/bo/miniconda3/envs/libero/lib/python3.8/site-packages/robomimic/utils/dataset.py:516: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pad_mask = pad_mask[:, None].astype(np.bool)\n",
      "/home/bo/miniconda3/envs/libero/lib/python3.8/site-packages/robomimic/utils/dataset.py:516: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pad_mask = pad_mask[:, None].astype(np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Epoch:   0 | train loss: 11.42 | time: 0.40\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: mujoco._structs.MjData, arg1: float) -> None\n\nInvoked with: <mujoco._structs.MjData object at 0x7f8845f43430>, array([ 0.00000000e+00, -1.38453941e-04, -1.76734475e-01, -3.65575975e-02,\n       -2.45711298e+00,  5.93502672e-03,  2.23521997e+00,  7.99383438e-01,\n        2.08330000e-02, -2.08330000e-02, -6.34828177e-02,  2.02062562e-01,\n        9.70000000e-01,  7.07106781e-01,  0.00000000e+00,  0.00000000e+00,\n        7.07106781e-01, -1.88730494e-01,  3.20384503e-01,  9.70000000e-01,\n        7.07106781e-01,  0.00000000e+00,  0.00000000e+00,  7.07106781e-01,\n        5.78595875e-02,  2.64329413e-02,  9.70000000e-01,  7.07106781e-01,\n        0.00000000e+00,  0.00000000e+00,  7.07106781e-01, -1.97384170e-01,\n        1.89135261e-01,  9.70000000e-01,  7.07106781e-01,  0.00000000e+00,\n        0.00000000e+00,  7.07106781e-01,  5.34091668e-02,  2.05182337e-01,\n        9.70000000e-01,  7.07106781e-01,  0.00000000e+00,  0.00000000e+00,\n        7.07106781e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_tasks):\n\u001b[1;32m     17\u001b[0m     algo\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m     s_fwd, l_fwd \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mlearn_one_task(datasets[i], i, benchmark, result_summary)\n\u001b[1;32m     19\u001b[0m     \u001b[39m# s_fwd is success rate AUC, when the agent learns the {0, e, 2e, ...} epochs\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# l_fwd is BC loss AUC, similar to s_fwd\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     result_summary[\u001b[39m\"\u001b[39m\u001b[39mS_fwd\u001b[39m\u001b[39m\"\u001b[39m][i] \u001b[39m=\u001b[39m s_fwd\n",
      "File \u001b[0;32m~/Desktop/tmp/libero-dev/libero/lifelong/algos/base.py:166\u001b[0m, in \u001b[0;36mSequential.learn_one_task\u001b[0;34m(self, dataset, task_id, benchmark, result_summary)\u001b[0m\n\u001b[1;32m    164\u001b[0m task_str \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mk\u001b[39m\u001b[39m{\u001b[39;00mtask_id\u001b[39m}\u001b[39;00m\u001b[39m_e\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39meval\u001b[39m.\u001b[39meval_every\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m sim_states \u001b[39m=\u001b[39m result_summary[task_str] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39meval\u001b[39m.\u001b[39msave_sim_states \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m success_rate \u001b[39m=\u001b[39m evaluate_one_task_success(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg,\n\u001b[1;32m    167\u001b[0m                                          \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    168\u001b[0m                                          task,\n\u001b[1;32m    169\u001b[0m                                          task_emb,\n\u001b[1;32m    170\u001b[0m                                          task_id,\n\u001b[1;32m    171\u001b[0m                                          sim_states\u001b[39m=\u001b[39;49msim_states,\n\u001b[1;32m    172\u001b[0m                                          task_str\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    173\u001b[0m successes\u001b[39m.\u001b[39mappend(success_rate)\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m prev_success_rate \u001b[39m<\u001b[39m success_rate:\n",
      "File \u001b[0;32m~/Desktop/tmp/libero-dev/libero/lifelong/metric.py:110\u001b[0m, in \u001b[0;36mevaluate_one_task_success\u001b[0;34m(cfg, algo, task, task_emb, task_id, sim_states, task_str)\u001b[0m\n\u001b[1;32m    108\u001b[0m steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    109\u001b[0m algo\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 110\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mset_init_state(init_states_)\n\u001b[1;32m    112\u001b[0m \u001b[39m# dummy actions [env_num, 7] all zeros for initial physics simulation\u001b[39;00m\n\u001b[1;32m    113\u001b[0m dummy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((env_num, \u001b[39m7\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/tmp/libero-dev/libero/libero/envs/env_wrapper.py:134\u001b[0m, in \u001b[0;36mControlEnv.set_init_state\u001b[0;34m(self, init_state)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_init_state\u001b[39m(\u001b[39mself\u001b[39m, init_state):\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregenerate_obs_from_state(init_state)\n",
      "File \u001b[0;32m~/Desktop/tmp/libero-dev/libero/libero/envs/env_wrapper.py:137\u001b[0m, in \u001b[0;36mControlEnv.regenerate_obs_from_state\u001b[0;34m(self, mujoco_state)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mregenerate_obs_from_state\u001b[39m(\u001b[39mself\u001b[39m, mujoco_state):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_state(mujoco_state)\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39msim\u001b[39m.\u001b[39mforward()\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_success()\n",
      "File \u001b[0;32m~/Desktop/tmp/libero-dev/libero/libero/envs/env_wrapper.py:125\u001b[0m, in \u001b[0;36mControlEnv.set_state\u001b[0;34m(self, mujoco_state)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_state\u001b[39m(\u001b[39mself\u001b[39m, mujoco_state):\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mset_state_from_flattened(mujoco_state)\n",
      "File \u001b[0;32m~/Desktop/robosuite/robosuite/utils/binding_utils.py:1167\u001b[0m, in \u001b[0;36mMjSim.set_state_from_flattened\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1164\u001b[0m state \u001b[39m=\u001b[39m MjSimState\u001b[39m.\u001b[39mfrom_flattened(value, \u001b[39mself\u001b[39m)\n\u001b[1;32m   1166\u001b[0m \u001b[39m# do this instead of @set_state to avoid extra copy of qpos and qvel\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mtime \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mtime\n\u001b[1;32m   1168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mqpos[:] \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mqpos\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mqvel[:] \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mqvel\n",
      "File \u001b[0;32m~/Desktop/robosuite/robosuite/utils/binding_utils.py:568\u001b[0m, in \u001b[0;36m_MjDataMeta.__new__.<locals>.<lambda>\u001b[0;34m(self, value, attr)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dct:\n\u001b[1;32m    566\u001b[0m     \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     fget \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, attr\u001b[39m=\u001b[39mattr: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, attr)\n\u001b[0;32m--> 568\u001b[0m     fset \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, value, attr\u001b[39m=\u001b[39mattr: \u001b[39msetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, attr, value)\n\u001b[1;32m    569\u001b[0m     \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     dct[attr] \u001b[39m=\u001b[39m \u001b[39mproperty\u001b[39m(fget, fset)\n",
      "\u001b[0;31mTypeError\u001b[0m: (): incompatible function arguments. The following argument types are supported:\n    1. (arg0: mujoco._structs.MjData, arg1: float) -> None\n\nInvoked with: <mujoco._structs.MjData object at 0x7f8845f43430>, array([ 0.00000000e+00, -1.38453941e-04, -1.76734475e-01, -3.65575975e-02,\n       -2.45711298e+00,  5.93502672e-03,  2.23521997e+00,  7.99383438e-01,\n        2.08330000e-02, -2.08330000e-02, -6.34828177e-02,  2.02062562e-01,\n        9.70000000e-01,  7.07106781e-01,  0.00000000e+00,  0.00000000e+00,\n        7.07106781e-01, -1.88730494e-01,  3.20384503e-01,  9.70000000e-01,\n        7.07106781e-01,  0.00000000e+00,  0.00000000e+00,  7.07106781e-01,\n        5.78595875e-02,  2.64329413e-02,  9.70000000e-01,  7.07106781e-01,\n        0.00000000e+00,  0.00000000e+00,  7.07106781e-01, -1.97384170e-01,\n        1.89135261e-01,  9.70000000e-01,  7.07106781e-01,  0.00000000e+00,\n        0.00000000e+00,  7.07106781e-01,  5.34091668e-02,  2.05182337e-01,\n        9.70000000e-01,  7.07106781e-01,  0.00000000e+00,  0.00000000e+00,\n        7.07106781e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
     ]
    }
   ],
   "source": [
    "create_experiment_dir(cfg)\n",
    "cfg.shape_meta = shape_meta\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"experiment directory is: \", cfg.experiment_dir)\n",
    "algo = safe_device(MyLifelongAlgo(n_tasks, cfg), cfg.device)\n",
    "\n",
    "result_summary = {\n",
    "    'L_conf_mat': np.zeros((n_tasks, n_tasks)),   # loss confusion matrix\n",
    "    'S_conf_mat': np.zeros((n_tasks, n_tasks)),   # success confusion matrix\n",
    "    'L_fwd'     : np.zeros((n_tasks,)),           # loss AUC, how fast the agent learns\n",
    "    'S_fwd'     : np.zeros((n_tasks,)),           # success AUC, how fast the agent succeeds\n",
    "}\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    algo.train()\n",
    "    s_fwd, l_fwd = algo.learn_one_task(datasets[i], i, benchmark, result_summary)\n",
    "    # s_fwd is success rate AUC, when the agent learns the {0, e, 2e, ...} epochs\n",
    "    # l_fwd is BC loss AUC, similar to s_fwd\n",
    "    result_summary[\"S_fwd\"][i] = s_fwd\n",
    "    result_summary[\"L_fwd\"][i] = l_fwd\n",
    "\n",
    "    if cfg.eval.eval:\n",
    "        algo.eval()\n",
    "        # we only evaluate on the past tasks: 0 .. i\n",
    "        L = evaluate_loss(cfg, algo, benchmark, datasets[:i+1]) # (i+1,)\n",
    "        S = evaluate_success(cfg, algo, benchmark, list(range((i+1)*gsz))) # (i+1,)\n",
    "        result_summary[\"L_conf_mat\"][i][:i+1] = L\n",
    "        result_summary[\"S_conf_mat\"][i][:i+1] = S\n",
    "\n",
    "        torch.save(result_summary, os.path.join(cfg.experiment_dir, f'result.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize loss and success rate curves on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize policy rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
